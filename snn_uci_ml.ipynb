{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "snn_uci_ml.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOt62YzTCGPJPLe7e8UU38e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josemoti1999/sysdl_project/blob/master/snn_uci_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLrF0p1XAAqt",
        "colab_type": "text"
      },
      "source": [
        "##Iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rItD4gWSdVId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed42ZG5q-njF",
        "colab_type": "code",
        "outputId": "f44d6178-95e7-403e-9e95-e9514645f30e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "iris = pd.read_csv('iris.csv')\n",
        "print(iris.shape)\n",
        "iris.sample(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>5.5</td>\n",
              "      <td>2.6</td>\n",
              "      <td>4.4</td>\n",
              "      <td>1.2</td>\n",
              "      <td>Iris-versicolor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>6.4</td>\n",
              "      <td>2.8</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.2</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>6.1</td>\n",
              "      <td>2.6</td>\n",
              "      <td>5.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>6.4</td>\n",
              "      <td>3.1</td>\n",
              "      <td>5.5</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>5.2</td>\n",
              "      <td>4.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>7.2</td>\n",
              "      <td>3.6</td>\n",
              "      <td>6.1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>4.9</td>\n",
              "      <td>2.4</td>\n",
              "      <td>3.3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Iris-versicolor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>5.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal_length  sepal_width  petal_length  petal_width          species\n",
              "90            5.5          2.6           4.4          1.2  Iris-versicolor\n",
              "132           6.4          2.8           5.6          2.2   Iris-virginica\n",
              "134           6.1          2.6           5.6          1.4   Iris-virginica\n",
              "137           6.4          3.1           5.5          1.8   Iris-virginica\n",
              "9             4.9          3.1           1.5          0.1      Iris-setosa\n",
              "147           6.5          3.0           5.2          2.0   Iris-virginica\n",
              "32            5.2          4.1           1.5          0.1      Iris-setosa\n",
              "109           7.2          3.6           6.1          2.5   Iris-virginica\n",
              "57            4.9          2.4           3.3          1.0  Iris-versicolor\n",
              "28            5.2          3.4           1.4          0.2      Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpFu3qe2-rd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mappings = {\n",
        "   'Iris-setosa': 0,\n",
        "   'Iris-versicolor': 1,\n",
        "   'Iris-virginica': 2\n",
        "}\n",
        "iris['species'] = iris['species'].apply(lambda x: mappings[x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtU2xFRz_rsR",
        "colab_type": "code",
        "outputId": "dc81841c-214d-48f1-fc92-6e7a25edacd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "iris.sample(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>6.3</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>7.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.1</td>\n",
              "      <td>2.3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>6.0</td>\n",
              "      <td>3.4</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>6.5</td>\n",
              "      <td>2.8</td>\n",
              "      <td>4.6</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.4</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal_length  sepal_width  petal_length  petal_width  species\n",
              "136           6.3          3.4           5.6          2.4        2\n",
              "135           7.7          3.0           6.1          2.3        2\n",
              "85            6.0          3.4           4.5          1.6        1\n",
              "54            6.5          2.8           4.6          1.5        1\n",
              "39            5.1          3.4           1.5          0.2        0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDYo7_JDJan1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris_train = iris.drop('species', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMmwS1nBDnLp",
        "colab_type": "text"
      },
      "source": [
        "Temporal encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFPPcEK4DpQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def temp_encoding(min_, max_, f, a=0, b=9):\n",
        "    #900 time steps\n",
        "    r=max_-min_\n",
        "    f_new=(((b-a)/r)*f) + (((a*max_)-(b*min_))/r)\n",
        "    return f_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMq0bo6uHuzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in iris_train.columns:\n",
        "    min_=iris[column].min()\n",
        "    max_=iris[column].max()\n",
        "    iris[column]=iris[column].apply(lambda x: round(temp_encoding(min_, max_, x), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znf3A2BkKFec",
        "colab_type": "code",
        "outputId": "f14f3b2c-b8fa-462b-c689-dd81485bd256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "iris.sample(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>8.25</td>\n",
              "      <td>3.75</td>\n",
              "      <td>8.54</td>\n",
              "      <td>7.50</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>3.00</td>\n",
              "      <td>8.25</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>3.25</td>\n",
              "      <td>3.75</td>\n",
              "      <td>5.34</td>\n",
              "      <td>5.25</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>3.00</td>\n",
              "      <td>2.25</td>\n",
              "      <td>5.19</td>\n",
              "      <td>4.12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.25</td>\n",
              "      <td>5.25</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal_length  sepal_width  petal_length  petal_width  species\n",
              "105          8.25         3.75          8.54         7.50        2\n",
              "33           3.00         8.25          0.61         0.38        0\n",
              "66           3.25         3.75          5.34         5.25        1\n",
              "90           3.00         2.25          5.19         4.12        1\n",
              "11           1.25         5.25          0.92         0.38        0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMaZrmqS_11X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = iris.drop('species', axis=1).values\n",
        "y = iris['species'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "y_test = torch.LongTensor(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIMT6eEDAcIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ANN(nn.Module):\n",
        "    def __init__(self, decay_multiplier=0.9, threshold=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decay_multiplier = decay_multiplier\n",
        "        self.threshold = threshold\n",
        "        self.prev_inner_1 = torch.zeros([16]).to(device)\n",
        "        self.prev_outer_1 = torch.zeros([16]).to(device)\n",
        "        self.prev_inner_2 = torch.zeros([12]).to(device)\n",
        "        self.prev_outer_2 = torch.zeros([12]).to(device)\n",
        "        self.prev_inner_3 = torch.zeros([3]).to(device)\n",
        "        self.prev_outer_3 = torch.zeros([3]).to(device)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=4, out_features=16)\n",
        "        self.fc2 = nn.Linear(in_features=16, out_features=12)\n",
        "        self.fc3 = nn.Linear(in_features=12, out_features=3)\n",
        " \n",
        "    def forward(self, x):\n",
        "        for i in range(900):\n",
        "            #input transformation\n",
        "            x=(x==i/100).float()\n",
        "            input_activation_1=self.fc1(x)\n",
        "            inner_excitation_1=input_activation_1+self.prev_inner_1*self.decay_multiplier\n",
        "            outer_excitation_1=(inner_excitation_1>self.threshold).float()\n",
        "            inner_excitation_1=inner_excitation_1 - (inner_excitation_1*outer_excitation_1)\n",
        "            self.prev_inner_1 = inner_excitation_1\n",
        "            \n",
        "            input_activation_2=self.fc2(outer_excitation_1)\n",
        "            inner_excitation_2=input_activation_2+self.prev_inner_2*self.decay_multiplier\n",
        "            outer_excitation_2=(inner_excitation_2>self.threshold).float()\n",
        "            inner_excitation_2=inner_excitation_2 - (inner_excitation_2*outer_excitation_2)\n",
        "            self.prev_inner_2 = inner_excitation_2\n",
        "\n",
        "            input_activation_3=self.fc3(outer_excitation_2)\n",
        "            inner_excitation_3=input_activation_3+self.prev_inner_3*self.decay_multiplier\n",
        "            self.prev_inner_3 = inner_excitation_3\n",
        "        \n",
        "        return inner_excitation_3 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDqfE_lnRdtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneticAlgorithm():\n",
        "    def __init__(self, model, target, data):\n",
        "        self.model=model\n",
        "        self.target=target\n",
        "        self.data=data\n",
        "\n",
        "    def mutate(self, population_size=20, mutation_power=0.02):\n",
        "        models_list=[]\n",
        "        models_list.append(self.model)\n",
        "        for _ in range(population_size):\n",
        "            child=copy.deepcopy(self.model)\n",
        "            for value in child.parameters():\n",
        "                tensor_shape=value.shape\n",
        "                noise=torch.randn(tensor_shape)\n",
        "                value+=mutation_power*noise\n",
        "            models_list.append(child)\n",
        "        return models_list\n",
        "\n",
        "    def find_best_model(self, population_size=20, mutation_power=0.02):\n",
        "        models_list=self.mutate(population_size, mutation_power)\n",
        "        loss_model_dict={}\n",
        "        for model in models_list:\n",
        "            model=model\n",
        "            output=model(self.data)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            loss=criterion(output, self.target)\n",
        "\n",
        "            loss_model_dict[model]=loss\n",
        "            for (model,loss) in sorted(loss_model_dict.items(), key=lambda x: x[1], reverse=False):\n",
        "                model_return=model\n",
        "                loss_return=loss\n",
        "                break\n",
        "        return model_return, loss_return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpf70rLgR5NE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "POPULATION_SIZE=100\n",
        "MUTATION_POWER=0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bolYgttXgd_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features=4, out_features=16)\n",
        "        self.fc2 = nn.Linear(in_features=16, out_features=12)\n",
        "        self.output = nn.Linear(in_features=12, out_features=3)\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQYrR5ysZlHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=ANN()\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jlnucHFBJfI",
        "colab_type": "code",
        "outputId": "0486c409-acd7-4685-cdc7-ae37afe1e048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 200\n",
        "loss_arr = []\n",
        "with torch.no_grad():\n",
        "    for i in range(epochs):\n",
        "        ga=GeneticAlgorithm(model, y_train, X_train)\n",
        "        model, loss = ga.find_best_model(POPULATION_SIZE, MUTATION_POWER)\n",
        "        loss_arr.append(loss)\n",
        "\n",
        "        print(f'Generation: {i} Loss: {loss}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generation: 0 Loss: 0.8747044205665588\n",
            "Generation: 1 Loss: 0.7306843400001526\n",
            "Generation: 2 Loss: 0.6197757124900818\n",
            "Generation: 3 Loss: 0.5756018161773682\n",
            "Generation: 4 Loss: 0.4995294213294983\n",
            "Generation: 5 Loss: 0.47288158535957336\n",
            "Generation: 6 Loss: 0.4597247540950775\n",
            "Generation: 7 Loss: 0.42335736751556396\n",
            "Generation: 8 Loss: 0.3988032042980194\n",
            "Generation: 9 Loss: 0.3988032042980194\n",
            "Generation: 10 Loss: 0.3318929374217987\n",
            "Generation: 11 Loss: 0.2974768877029419\n",
            "Generation: 12 Loss: 0.28521817922592163\n",
            "Generation: 13 Loss: 0.281078964471817\n",
            "Generation: 14 Loss: 0.281078964471817\n",
            "Generation: 15 Loss: 0.24107986688613892\n",
            "Generation: 16 Loss: 0.2352432906627655\n",
            "Generation: 17 Loss: 0.23092317581176758\n",
            "Generation: 18 Loss: 0.23092317581176758\n",
            "Generation: 19 Loss: 0.23092317581176758\n",
            "Generation: 20 Loss: 0.23092317581176758\n",
            "Generation: 21 Loss: 0.23092317581176758\n",
            "Generation: 22 Loss: 0.22041819989681244\n",
            "Generation: 23 Loss: 0.21217523515224457\n",
            "Generation: 24 Loss: 0.2073754519224167\n",
            "Generation: 25 Loss: 0.19350910186767578\n",
            "Generation: 26 Loss: 0.1684798300266266\n",
            "Generation: 27 Loss: 0.1684798300266266\n",
            "Generation: 28 Loss: 0.15040990710258484\n",
            "Generation: 29 Loss: 0.15040990710258484\n",
            "Generation: 30 Loss: 0.1325000673532486\n",
            "Generation: 31 Loss: 0.1325000673532486\n",
            "Generation: 32 Loss: 0.1325000673532486\n",
            "Generation: 33 Loss: 0.1325000673532486\n",
            "Generation: 34 Loss: 0.1325000673532486\n",
            "Generation: 35 Loss: 0.1325000673532486\n",
            "Generation: 36 Loss: 0.1325000673532486\n",
            "Generation: 37 Loss: 0.1325000673532486\n",
            "Generation: 38 Loss: 0.1325000673532486\n",
            "Generation: 39 Loss: 0.10561539232730865\n",
            "Generation: 40 Loss: 0.10561539232730865\n",
            "Generation: 41 Loss: 0.10561539232730865\n",
            "Generation: 42 Loss: 0.10561539232730865\n",
            "Generation: 43 Loss: 0.10561539232730865\n",
            "Generation: 44 Loss: 0.10561539232730865\n",
            "Generation: 45 Loss: 0.10561539232730865\n",
            "Generation: 46 Loss: 0.10561539232730865\n",
            "Generation: 47 Loss: 0.10561539232730865\n",
            "Generation: 48 Loss: 0.10561539232730865\n",
            "Generation: 49 Loss: 0.10561539232730865\n",
            "Generation: 50 Loss: 0.10088184475898743\n",
            "Generation: 51 Loss: 0.10088184475898743\n",
            "Generation: 52 Loss: 0.10088184475898743\n",
            "Generation: 53 Loss: 0.10088184475898743\n",
            "Generation: 54 Loss: 0.10088184475898743\n",
            "Generation: 55 Loss: 0.10088184475898743\n",
            "Generation: 56 Loss: 0.10088184475898743\n",
            "Generation: 57 Loss: 0.10088184475898743\n",
            "Generation: 58 Loss: 0.10088184475898743\n",
            "Generation: 59 Loss: 0.10088184475898743\n",
            "Generation: 60 Loss: 0.10088184475898743\n",
            "Generation: 61 Loss: 0.10088184475898743\n",
            "Generation: 62 Loss: 0.0976504534482956\n",
            "Generation: 63 Loss: 0.0976504534482956\n",
            "Generation: 64 Loss: 0.0976504534482956\n",
            "Generation: 65 Loss: 0.0976504534482956\n",
            "Generation: 66 Loss: 0.0976504534482956\n",
            "Generation: 67 Loss: 0.0976504534482956\n",
            "Generation: 68 Loss: 0.0976504534482956\n",
            "Generation: 69 Loss: 0.0976504534482956\n",
            "Generation: 70 Loss: 0.09413563460111618\n",
            "Generation: 71 Loss: 0.09024672955274582\n",
            "Generation: 72 Loss: 0.08959805965423584\n",
            "Generation: 73 Loss: 0.08959805965423584\n",
            "Generation: 74 Loss: 0.08959805965423584\n",
            "Generation: 75 Loss: 0.08959805965423584\n",
            "Generation: 76 Loss: 0.08959805965423584\n",
            "Generation: 77 Loss: 0.08959805965423584\n",
            "Generation: 78 Loss: 0.08959805965423584\n",
            "Generation: 79 Loss: 0.08959805965423584\n",
            "Generation: 80 Loss: 0.08959805965423584\n",
            "Generation: 81 Loss: 0.08959805965423584\n",
            "Generation: 82 Loss: 0.08959805965423584\n",
            "Generation: 83 Loss: 0.08177033066749573\n",
            "Generation: 84 Loss: 0.08177033066749573\n",
            "Generation: 85 Loss: 0.08177033066749573\n",
            "Generation: 86 Loss: 0.08177033066749573\n",
            "Generation: 87 Loss: 0.08177033066749573\n",
            "Generation: 88 Loss: 0.08177033066749573\n",
            "Generation: 89 Loss: 0.08177033066749573\n",
            "Generation: 90 Loss: 0.08177033066749573\n",
            "Generation: 91 Loss: 0.08177033066749573\n",
            "Generation: 92 Loss: 0.08177033066749573\n",
            "Generation: 93 Loss: 0.08177033066749573\n",
            "Generation: 94 Loss: 0.08177033066749573\n",
            "Generation: 95 Loss: 0.08177033066749573\n",
            "Generation: 96 Loss: 0.08177033066749573\n",
            "Generation: 97 Loss: 0.08177033066749573\n",
            "Generation: 98 Loss: 0.08177033066749573\n",
            "Generation: 99 Loss: 0.08177033066749573\n",
            "Generation: 100 Loss: 0.08177033066749573\n",
            "Generation: 101 Loss: 0.08177033066749573\n",
            "Generation: 102 Loss: 0.08177033066749573\n",
            "Generation: 103 Loss: 0.08177033066749573\n",
            "Generation: 104 Loss: 0.08177033066749573\n",
            "Generation: 105 Loss: 0.08177033066749573\n",
            "Generation: 106 Loss: 0.08177033066749573\n",
            "Generation: 107 Loss: 0.08177033066749573\n",
            "Generation: 108 Loss: 0.08177033066749573\n",
            "Generation: 109 Loss: 0.08177033066749573\n",
            "Generation: 110 Loss: 0.08177033066749573\n",
            "Generation: 111 Loss: 0.08177033066749573\n",
            "Generation: 112 Loss: 0.07768616825342178\n",
            "Generation: 113 Loss: 0.07768616825342178\n",
            "Generation: 114 Loss: 0.07768616825342178\n",
            "Generation: 115 Loss: 0.07768616825342178\n",
            "Generation: 116 Loss: 0.07768616825342178\n",
            "Generation: 117 Loss: 0.07768616825342178\n",
            "Generation: 118 Loss: 0.07768616825342178\n",
            "Generation: 119 Loss: 0.07768616825342178\n",
            "Generation: 120 Loss: 0.07768616825342178\n",
            "Generation: 121 Loss: 0.07768616825342178\n",
            "Generation: 122 Loss: 0.07768616825342178\n",
            "Generation: 123 Loss: 0.07768616825342178\n",
            "Generation: 124 Loss: 0.07392729073762894\n",
            "Generation: 125 Loss: 0.07392729073762894\n",
            "Generation: 126 Loss: 0.07392729073762894\n",
            "Generation: 127 Loss: 0.07392729073762894\n",
            "Generation: 128 Loss: 0.07392729073762894\n",
            "Generation: 129 Loss: 0.06275122612714767\n",
            "Generation: 130 Loss: 0.06275122612714767\n",
            "Generation: 131 Loss: 0.06275122612714767\n",
            "Generation: 132 Loss: 0.06275122612714767\n",
            "Generation: 133 Loss: 0.06275122612714767\n",
            "Generation: 134 Loss: 0.06275122612714767\n",
            "Generation: 135 Loss: 0.060862548649311066\n",
            "Generation: 136 Loss: 0.060862548649311066\n",
            "Generation: 137 Loss: 0.060862548649311066\n",
            "Generation: 138 Loss: 0.060862548649311066\n",
            "Generation: 139 Loss: 0.060862548649311066\n",
            "Generation: 140 Loss: 0.060862548649311066\n",
            "Generation: 141 Loss: 0.060862548649311066\n",
            "Generation: 142 Loss: 0.060862548649311066\n",
            "Generation: 143 Loss: 0.060862548649311066\n",
            "Generation: 144 Loss: 0.05946982651948929\n",
            "Generation: 145 Loss: 0.05946982651948929\n",
            "Generation: 146 Loss: 0.05946982651948929\n",
            "Generation: 147 Loss: 0.05946982651948929\n",
            "Generation: 148 Loss: 0.05946982651948929\n",
            "Generation: 149 Loss: 0.05946982651948929\n",
            "Generation: 150 Loss: 0.05946982651948929\n",
            "Generation: 151 Loss: 0.05946982651948929\n",
            "Generation: 152 Loss: 0.05946982651948929\n",
            "Generation: 153 Loss: 0.05946982651948929\n",
            "Generation: 154 Loss: 0.05946982651948929\n",
            "Generation: 155 Loss: 0.05946982651948929\n",
            "Generation: 156 Loss: 0.05946982651948929\n",
            "Generation: 157 Loss: 0.05946982651948929\n",
            "Generation: 158 Loss: 0.05946982651948929\n",
            "Generation: 159 Loss: 0.05946982651948929\n",
            "Generation: 160 Loss: 0.05946982651948929\n",
            "Generation: 161 Loss: 0.05946982651948929\n",
            "Generation: 162 Loss: 0.05946982651948929\n",
            "Generation: 163 Loss: 0.05946982651948929\n",
            "Generation: 164 Loss: 0.05946982651948929\n",
            "Generation: 165 Loss: 0.05946982651948929\n",
            "Generation: 166 Loss: 0.05946982651948929\n",
            "Generation: 167 Loss: 0.05946982651948929\n",
            "Generation: 168 Loss: 0.05946982651948929\n",
            "Generation: 169 Loss: 0.05946982651948929\n",
            "Generation: 170 Loss: 0.05946982651948929\n",
            "Generation: 171 Loss: 0.05946982651948929\n",
            "Generation: 172 Loss: 0.05946982651948929\n",
            "Generation: 173 Loss: 0.05946982651948929\n",
            "Generation: 174 Loss: 0.05946982651948929\n",
            "Generation: 175 Loss: 0.05946982651948929\n",
            "Generation: 176 Loss: 0.05946982651948929\n",
            "Generation: 177 Loss: 0.05946982651948929\n",
            "Generation: 178 Loss: 0.05946982651948929\n",
            "Generation: 179 Loss: 0.05946982651948929\n",
            "Generation: 180 Loss: 0.05946982651948929\n",
            "Generation: 181 Loss: 0.05946982651948929\n",
            "Generation: 182 Loss: 0.05946982651948929\n",
            "Generation: 183 Loss: 0.05946982651948929\n",
            "Generation: 184 Loss: 0.05946982651948929\n",
            "Generation: 185 Loss: 0.05946982651948929\n",
            "Generation: 186 Loss: 0.05946982651948929\n",
            "Generation: 187 Loss: 0.05946982651948929\n",
            "Generation: 188 Loss: 0.05946982651948929\n",
            "Generation: 189 Loss: 0.05946982651948929\n",
            "Generation: 190 Loss: 0.05946982651948929\n",
            "Generation: 191 Loss: 0.05946982651948929\n",
            "Generation: 192 Loss: 0.05946982651948929\n",
            "Generation: 193 Loss: 0.05946982651948929\n",
            "Generation: 194 Loss: 0.05946982651948929\n",
            "Generation: 195 Loss: 0.05946982651948929\n",
            "Generation: 196 Loss: 0.05946982651948929\n",
            "Generation: 197 Loss: 0.05946982651948929\n",
            "Generation: 198 Loss: 0.057645637542009354\n",
            "Generation: 199 Loss: 0.057645637542009354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZifrnuRBPYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = []\n",
        "with torch.no_grad():\n",
        "   for val in X_test:\n",
        "       y_hat = model.forward(val)\n",
        "       preds.append(y_hat.argmax().item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JviJEA2KCBbQ",
        "colab_type": "code",
        "outputId": "49e8d75c-f53b-4a10-8121-e36e38e35f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "df = pd.DataFrame({'Y': y_test, 'YHat': preds})\n",
        "df['Correct'] = [1 if corr == pred else 0 for corr, pred in zip(df['Y'], df['YHat'])]\n",
        "df.sample()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y</th>\n",
              "      <th>YHat</th>\n",
              "      <th>Correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Y  YHat  Correct\n",
              "12  0     0        1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Z3kRajCHOD",
        "colab_type": "code",
        "outputId": "4ec2e62e-efab-4c90-e67b-11f4035a7fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df['Correct'].sum() / len(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9666666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    }
  ]
}